{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Home Depot Decor Case\n",
    "\n",
    "Getting Started  |  Data Prep  |  Data Exploration  |  **Preprocessing**  |  Model Tuning  |  Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T03:41:18.698069Z",
     "start_time": "2019-07-19T03:41:18.690039Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T01:54:33.267387Z",
     "start_time": "2019-07-19T01:54:33.257655Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "sys.path.insert(0, '../modules')\n",
    "\n",
    "\n",
    "# now read in new functions\n",
    "from helpers import read_in_dataset, get_num_of_levels, flatten_categories, search_cons_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T01:54:39.242744Z",
     "start_time": "2019-07-19T01:54:37.383234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in product related data\n",
    "verbose_opt = False\n",
    "catalog = read_in_dataset('Decor_catalog.csv', verbose=verbose_opt)\n",
    "prod_desc = read_in_dataset('Product_name_description.csv', verbose=verbose_opt)\n",
    "prod_engagement = read_in_dataset('Product_engagement.csv', verbose=verbose_opt)\n",
    "\n",
    "# Read in search related data\n",
    "navigations = read_in_dataset('Visual_navigations.csv', verbose=verbose_opt)\n",
    "search_imp = read_in_dataset('Search_impression.csv', verbose=verbose_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess Product Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T03:31:52.195550Z",
     "start_time": "2019-07-19T03:31:52.187442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add Stopword List\n",
    "# Define List of Stop Words\n",
    "new_stop_words = ['in', 'sq','ft', 'yd', 'cm', 'mm','gal','lb' ,'lbs','qt','oz', 'h', 'w', 'ii', 'x']\n",
    "\n",
    "stop_words = set(stopwords.words('english') + new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T03:31:56.362135Z",
     "start_time": "2019-07-19T03:31:56.358200Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert Product Name to Array\n",
    "names = prod_desc['Product_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T05:08:35.023536Z",
     "start_time": "2019-07-19T05:08:35.017581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(docs, stop_words=stop_words):\n",
    "    '''\n",
    "    Process docs\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tokenized list of docs\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for doc in docs:\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        yield([lemmatizer.lemmatize(token) for token in tokens if not token in stop_words])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T05:19:56.535573Z",
     "start_time": "2019-07-19T05:19:43.170607Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_names = list(preprocess_text(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:00:54.431895Z",
     "start_time": "2019-07-19T07:00:54.425771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def join_tokens(clean_names):\n",
    "    for name in clean_names:\n",
    "        yield ' '.join([token for token in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:01:29.138740Z",
     "start_time": "2019-07-19T07:01:28.898132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names_corpus = list(join_tokens(clean_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build Bigram/Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T05:09:30.721930Z",
     "start_time": "2019-07-19T05:09:10.023713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(clean_names, min_count=25)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[clean_names], min_count=15) \n",
    "\n",
    "# Names as a trigram/bigram\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T05:20:09.192802Z",
     "start_time": "2019-07-19T05:20:09.186394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trigram_model(corpus, threshholds=(25,15), verbose=False):\n",
    "    '''\n",
    "    Build trigram model\n",
    "    '''\n",
    "\n",
    "    bigram = gensim.models.Phrases(clean_names,\n",
    "                                    min_count=threshholds[0])\n",
    "\n",
    "    trigram = gensim.models.Phrases(bigram[clean_names],\n",
    "                                    min_count=threshholds[1])\n",
    "\n",
    "    # trigram/bigram model\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    corpus_new = [trigram_model[bigram_model[doc]] for doc in corpus]\n",
    "\n",
    "    if verbose:\n",
    "        for doc in corpus[0:5]:\n",
    "            print(f'{\" \".join(trigram_model[bigram_model[doc]]) } \\n')\n",
    "                  \n",
    "    return corpus_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T05:21:10.338172Z",
     "start_time": "2019-07-19T05:20:36.077204Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jaime/miniconda3/envs/thddecor/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concrete solid block \n",
      "\n",
      "hummingbird stencil \n",
      "\n",
      "acrylic clear white dry_erase_board \n",
      "\n",
      "clear white boom dry_erase_board \n",
      "\n",
      "coconut charcoal \n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_new = trigram_model(clean_names, threshholds=(25,15), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocess Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T04:35:07.467386Z",
     "start_time": "2019-07-19T04:35:07.454260Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Number of Searches: 258\n",
      "Number of Consolidated Searches: 96\n",
      "Percent Reduction: 27.1%\n"
     ]
    }
   ],
   "source": [
    "# Make array\n",
    "searches = search_imp['Search_term'].values\n",
    "\n",
    "# Clean Search Terms\n",
    "clean_searches = list(preprocess_text(searches))\n",
    "\n",
    "# Get Status\n",
    "search_cons_status(clean_searches, searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:37:23.532147Z",
     "start_time": "2019-07-19T06:37:23.527613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens\n",
    "search_tokens = [search.split() for search in clean_searches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:41:59.465360Z",
     "start_time": "2019-07-19T06:41:36.744070Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jaime/miniconda3/envs/thddecor/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee \n",
      "\n",
      "bag chair \n",
      "\n",
      "kitchen wall tile \n",
      "\n",
      "mirror tile \n",
      "\n",
      "entryway \n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_trigrams = trigram_model(search_tokens, threshholds=(25,15), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:46:52.161823Z",
     "start_time": "2019-07-19T06:46:52.157825Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:46:53.007566Z",
     "start_time": "2019-07-19T06:46:52.992194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_vec, search_matrix = vectorize(clean_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:47:22.808035Z",
     "start_time": "2019-07-19T06:47:22.799545Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<354x199 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 698 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:06:18.438716Z",
     "start_time": "2019-07-19T07:06:15.739883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prod_vec, prod_matrix = vectorize(names_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:03.754967Z",
     "start_time": "2019-07-19T07:51:03.742996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "        \n",
    "    def preprocess(self, corpus):\n",
    "        '''\n",
    "        Process documents\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * corpus - list of documents\n",
    "        * stop_words = set of stopwords\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Each document as a list of tokens (iterator)\n",
    "\n",
    "        e.g., clean_corpus = list(preprocess(corpus))\n",
    "        '''\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        for doc in corpus:\n",
    "            tokens = gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "            yield([lemmatizer.lemmatize(token) for token in tokens if not token in stop_words])\n",
    "    \n",
    "    \n",
    "    def trigram_model(self, corpus_tokens, threshholds=(25,15), verbose=False):\n",
    "        '''\n",
    "        Build trigram model\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * corpus - list of documents (as tokens)\n",
    "        * stop_words = set of stopwords\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        '''\n",
    "\n",
    "        bigram = gensim.models.Phrases(corpus_tokens,\n",
    "                                        min_count=threshholds[0])\n",
    "\n",
    "        trigram = gensim.models.Phrases(bigram[corpus_tokens],\n",
    "                                        min_count=threshholds[1])\n",
    "\n",
    "        # trigram/bigram model\n",
    "        bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        corpus_new = [trigram_model[bigram_model[doc]] for doc in corpus_tokens]\n",
    "\n",
    "        if verbose:\n",
    "            for doc in corpus_new[0:5]:\n",
    "                print(f'{\" \".join(trigram_model[bigram_model[doc]]) } \\n')\n",
    "\n",
    "        return corpus_new\n",
    "                      \n",
    "    def vectorize(self, corpus, format='tokens'):\n",
    "                      \n",
    "        if format == 'tokens':    \n",
    "            corpus = self.tokens2corpus(corpus)\n",
    "    \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        return vectorizer, matrix\n",
    "\n",
    "\n",
    "    def tokens2corpus(self, tokenized_docs):\n",
    "        '''\n",
    "        Helper fuction to convert from tokens to text\n",
    "        '''\n",
    "        for doc in tokenized_docs:\n",
    "            yield ' '.join([token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Testing Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:04.706363Z",
     "start_time": "2019-07-19T07:51:04.701928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tp = TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:05.201313Z",
     "start_time": "2019-07-19T07:51:05.198153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search = tp(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:06.610371Z",
     "start_time": "2019-07-19T07:51:06.597143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_searches = list(search.preprocess(searches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:08.187428Z",
     "start_time": "2019-07-19T07:51:08.164037Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee \n",
      "\n",
      "bag chair \n",
      "\n",
      "kitchen wall tile \n",
      "\n",
      "mirror tile \n",
      "\n",
      "entryway \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jaime/miniconda3/envs/thddecor/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "searches_trigram = search.trigram_model(clean_searches, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:13.114489Z",
     "start_time": "2019-07-19T07:51:13.106608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "searches_vec, searches_matrix = search.vectorize(searches_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**To Explore**\n",
    "- Language detection \n",
    "- Snowball stemmer vs Lemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:00:15.585780Z",
     "start_time": "2019-07-16T21:00:12.331261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Make names df\n",
    "names_df= prod_desc_cat.loc[:, ['SKU_ID','Product_name']]\n",
    "names_df.set_index('Product_name', inplace=True)\n",
    "names_df['clean_names'] = clean_names\n",
    "names_df['clean_names'] = names_df['clean_names'].str.join(' ')\n",
    "\n",
    "\n",
    "#Make queries df\n",
    "queries_df = search_prod_levels.loc[:, ['Search_term']]\n",
    "queries_df['clean_queries'] = clean_queries\n",
    "queries_df = queries_df.drop_duplicates('Search_term')\n",
    "queries_df.set_index('Search_term', inplace=True)\n",
    "queries_df['clean_queries'] = queries_df['clean_queries'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:00:31.022546Z",
     "start_time": "2019-07-16T21:00:21.417756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words=stop_words)\n",
    "tfidf_matrix_names = tf.fit_transform(names_df['clean_names'])\n",
    "tfidf_matrix_queries = tf.fit_transform(queries_df['clean_queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-16T21:07:55.425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Cosine similarity\n",
    "cosine_similarities = linear_kernel(tfidf_matrix_names, tfidf_matrix_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(names_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:07:45.336989Z",
     "start_time": "2019-07-16T21:06:06.381Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recommendations(name, cosine_similarities = cosine_similarities):\n",
    "    \n",
    "    recommended_products = []\n",
    "    \n",
    "    # gettin the index of the hotel that matches the name\n",
    "    idx = indices[indices == name].index[0]\n",
    "\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar hotels except itself\n",
    "    top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    \n",
    "    # populating the list with the names of the top 10 matching hotels\n",
    "    for i in top_10_indexes:\n",
    "        recommended_products.append(list(df.index)[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "recommendations('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create Preprocessor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "    def __init__(self, cols_to_filter=None):\n",
    "        \n",
    "        self.cols_to_filter = cols_to_filter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"learn any information from the training data we may need to transform the test data\"\"\"\n",
    "        \n",
    "        # learn from the training data and return the class itself. \n",
    "        # allows you to chain fit and predict methods like \n",
    "        \n",
    "        # > p = preprocessor()\n",
    "        # > p.fit(X).transform(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"transform the training or test data\"\"\"\n",
    "        # transform the training or test data based on class attributes learned in the `fit` step\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Define Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:thddecor]",
   "language": "python",
   "name": "conda-env-thddecor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
