{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Home Depot Decor Case\n",
    "\n",
    "Getting Started  |  Data Prep  |  Data Exploration  |  **Preprocessing**  |  Model Tuning  |  Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T07:26:02.392724Z",
     "start_time": "2019-07-22T07:26:02.385916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#from langdetect import detect\n",
    "#from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:51:58.046165Z",
     "start_time": "2019-07-21T20:51:58.042097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "sys.path.insert(0, '../modules')\n",
    "\n",
    "\n",
    "# now read in new functions\n",
    "from helpers import read_in_dataset, get_num_of_levels, flatten_categories, search_cons_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T06:20:40.956399Z",
     "start_time": "2019-07-22T06:20:38.679225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read in product related data\n",
    "verbose_opt = False\n",
    "catalog = read_in_dataset('Decor_catalog.csv', verbose=verbose_opt)\n",
    "prod_desc = read_in_dataset('Product_name_description.csv', verbose=verbose_opt)\n",
    "prod_engagement = read_in_dataset('Product_engagement.csv', verbose=verbose_opt)\n",
    "\n",
    "# Read in search related data\n",
    "navigations = read_in_dataset('Visual_navigations.csv', verbose=verbose_opt)\n",
    "search_imp = read_in_dataset('Search_impression.csv', verbose=verbose_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T06:20:57.526305Z",
     "start_time": "2019-07-22T06:20:57.517895Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search_term</th>\n",
       "      <th>Impression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>203054703;207061099;305561354;305561469;301692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bag chair</td>\n",
       "      <td>305573411;305608772;301092388;301092383;301092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen wall tile</td>\n",
       "      <td>205140711;302603437;205762409;204923728;204337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mirror tile</td>\n",
       "      <td>305696621;304142073;304142126;304142039;303058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entryway</td>\n",
       "      <td>203532713;203532652;300750153;302042988;302042...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Search_term                                         Impression\n",
       "0             coffee  203054703;207061099;305561354;305561469;301692...\n",
       "1          bag chair  305573411;305608772;301092388;301092383;301092...\n",
       "2  kitchen wall tile  205140711;302603437;205762409;204923728;204337...\n",
       "3        mirror tile  305696621;304142073;304142126;304142039;303058...\n",
       "4           entryway  203532713;203532652;300750153;302042988;302042..."
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_imp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:13:23.437017Z",
     "start_time": "2019-07-22T08:13:23.434042Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Build Text Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:26:29.441401Z",
     "start_time": "2019-07-22T08:26:29.425633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, search_imp=None, prod_desc=None, catalog=None):\n",
    "        self.search_imp = search_imp\n",
    "        self.prod_desc = prod_desc\n",
    "        self.catalog = catalog\n",
    "        \n",
    "        self.stop_words_lst = ['in', 'sq','ft', 'yd', 'cm', 'mm','gal','lb' ,'lbs','qt','oz', 'h', 'w', 'ii', 'x']\n",
    "        self.stop_words = list(set(stopwords.words('english') + new_stop_words))\n",
    "        \n",
    "    def read_in_dataset(self, data, data_folder='raw', verbose=False):\n",
    "        '''\n",
    "        Read in dataset (csv format) to pandas dataframe\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * dataset - string with dataset filename\n",
    "        * data_folder - string with either raw or processed\n",
    "        * verbose - True will print intormation about the dataset\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        a pandas dataframe\n",
    "        '''\n",
    "        df = pd.read_csv('../data/{}/{}.csv'.format(data_folder, data))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def clean_text(self, df, corpus_col):\n",
    "        '''Call preprocessor generator object'''\n",
    "\n",
    "        corpus = df.loc[:, corpus_col].tolist()\n",
    "        \n",
    "        return list(self.preprocess_text(corpus))\n",
    "        \n",
    "\n",
    "    def preprocess_text(self, docs):\n",
    "        '''\n",
    "        Process docs\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tokenized list of docs\n",
    "        '''\n",
    "        #docs = docs.values\n",
    "        method='lemmatizer'\n",
    "        if method == 'lemmatizer':\n",
    "            lemma = nltk.stem.WordNetLemmatizer()\n",
    "            root = lemma.lemmatize\n",
    "        elif method == 'stemmer':\n",
    "            stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "            root = stemmer.stem\n",
    "\n",
    "        for doc in docs:\n",
    "            tokens = gensim.utils.simple_preprocess(doc)\n",
    "            yield(' '.join([root(token) for token in tokens if not token in self.stop_words]))\n",
    "        \n",
    "    def doc2tokens(corpus):\n",
    "        return corpus.str.split()\n",
    "    \n",
    "    def compare_clean_searches(self, clean_searches, raw_search):\n",
    "        '''Compare cleaned search queries to raw text'''\n",
    "        st_compare = pd.DataFrame({'raw_search': raw_search['Search_term'], 'cleaned_search':clean_searches}).sort_values(by='raw_search')\n",
    "        return st_compare.groupby('cleaned_search')['raw_search'].apply(list)\n",
    "\n",
    "    def add_stopword(self, new_stopword):\n",
    "        self.stop_words.append(new_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Search Term Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Preprocess Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(docs, method='lemmatizer'):\n",
    "    '''\n",
    "    Process docs\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tokenized list of docs\n",
    "    '''\n",
    "    docs = docs.values\n",
    "    stop_words = ['I']\n",
    "    if method == 'lemmatizer':\n",
    "        lemma = nltk.stem.WordNetLemmatizer()\n",
    "        root = lemma.lemmatize\n",
    "        \n",
    "    elif method == 'stemmer':\n",
    "        stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "        root = stemmer.stem\n",
    "\n",
    "    for doc in docs:\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        yield(' '.join([root(token) for token in tokens if not token in stop_words]))\n",
    "        \n",
    "        \n",
    "def doc2tokens(corpus):\n",
    "    '''pandas series (docs) to tokens'''\n",
    "        return corpus.str.split()\n",
    "    \n",
    "    \n",
    "def cons_search_terms(clean_searches=clean_searches, raw_search=search_imp):\n",
    "    '''Compare cleaned search queries to raw text'''\n",
    "    st_compare = pd.DataFrame({'raw_search': raw_search['Search_term'], 'cleaned_search':clean_searches}).sort_values(by='raw_search')\n",
    "    return st_compare.groupby('cleaned_search')['raw_search'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T07:56:06.014424Z",
     "start_time": "2019-07-22T07:56:05.937261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "search_imp = read_in_dataset('Search_impression.csv', verbose=verbose_opt)\n",
    "\n",
    "# Clean Search Terms\n",
    "clean_searches = pd.Series(list(preprocess_text(search_imp['Search_term'])))\n",
    "\n",
    "# tokens\n",
    "search_tokens = doc2tokens(clean_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ngram model\n",
    "#search_trigrams = trigram_model(search_tokens, threshholds=(25,15), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:02:51.724074Z",
     "start_time": "2019-07-22T08:02:51.682070Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_search\n",
       "accent table                [accent table, accent tables]\n",
       "adhesive backsplash                 [adhesive backsplash]\n",
       "adhesive tile backsplash       [adhesive tile backsplash]\n",
       "arm chair                                     [arm chair]\n",
       "armchair                                       [armchair]\n",
       "Name: raw_search, dtype: object"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare clean searches to raw searches\n",
    "\n",
    "cons_search_terms().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:52:44.437695Z",
     "start_time": "2019-07-21T20:52:44.431395Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Explore Search Term Similarity through collaborative filtering\n",
    "\n",
    "# Copy df\n",
    "search_compare_df = search_imp\n",
    "\n",
    "#add cleaned text \n",
    "search_compare_df['cleaned_search'] = clean_searches_series\n",
    "\n",
    "#split Impressions\n",
    "search_compare_df['Impression'] = search_compare_df['Impression'].str.replace(';', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:53:45.067832Z",
     "start_time": "2019-07-21T20:53:45.001785Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_search</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accent table</th>\n",
       "      <td>302192857 303444081 302267969 301285388 305852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive backsplash</th>\n",
       "      <td>202541460 206876241 204860117 301094937 207210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive tile backsplash</th>\n",
       "      <td>203601385 204208646 202823734 203601363 206110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arm chair</th>\n",
       "      <td>205409569 207020841 206374902 304059401 302765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armchair</th>\n",
       "      <td>205409569 206374902 207020841 203195567 205181...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Impression\n",
       "cleaned_search                                                             \n",
       "accent table              302192857 303444081 302267969 301285388 305852...\n",
       "adhesive backsplash       202541460 206876241 204860117 301094937 207210...\n",
       "adhesive tile backsplash  203601385 204208646 202823734 203601363 206110...\n",
       "arm chair                 205409569 207020841 206374902 304059401 302765...\n",
       "armchair                  205409569 206374902 207020841 203195567 205181..."
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupby cleaned search\n",
    "search_imp_clean = pd.DataFrame(search_compare_df.groupby('cleaned_search')['Impression'].apply(sum))\n",
    "\n",
    "search_imp_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:35:19.165510Z",
     "start_time": "2019-07-21T20:35:19.155554Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_search\n",
       "accent table                {302768792, 301239142, 306775214, 301654365, 2...\n",
       "adhesive backsplash         {206885906, 301094798, 303617520, 302991748, 2...\n",
       "adhesive tile backsplash    {205972972, 302681744, 203601363, 100521695, 2...\n",
       "arm chair                   {206890535, 207020841, 306053407, 205786340, 3...\n",
       "armchair                    {302796853, 207020841, 204083379, 205786340, 2...\n",
       "Name: Impression, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search_imp_clean_set = search_imp_clean['Impression'].apply(set)\n",
    "\n",
    "#search_imp_clean_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:57:23.905632Z",
     "start_time": "2019-07-21T20:57:23.900852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imp_docs = search_imp_clean['Impression'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:57:36.404303Z",
     "start_time": "2019-07-21T20:57:36.382659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Count vec\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "vec_imp = CountVectorizer(binary=True)\n",
    "imp_matrix = vec_imp.fit_transform(imp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:04:00.679311Z",
     "start_time": "2019-07-21T22:04:00.655886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "\n",
    "tfvec_imp = TfidfVectorizer(binary=True)\n",
    "tfimp_matrix = tfvec_imp.fit_transform(imp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:58:23.078374Z",
     "start_time": "2019-07-21T20:58:23.041805Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100012014</th>\n",
       "      <th>100022800</th>\n",
       "      <th>100023109</th>\n",
       "      <th>100023973</th>\n",
       "      <th>100036137</th>\n",
       "      <th>100044505</th>\n",
       "      <th>100048075</th>\n",
       "      <th>100051570</th>\n",
       "      <th>100061089</th>\n",
       "      <th>100074869</th>\n",
       "      <th>...</th>\n",
       "      <th>307717049</th>\n",
       "      <th>307717219</th>\n",
       "      <th>307717221</th>\n",
       "      <th>307727052</th>\n",
       "      <th>307833295</th>\n",
       "      <th>307920434</th>\n",
       "      <th>307939445</th>\n",
       "      <th>307939707</th>\n",
       "      <th>307940057</th>\n",
       "      <th>307940314</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_search</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accent table</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive backsplash</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive tile backsplash</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arm chair</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armchair</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          100012014  100022800  100023109  100023973  \\\n",
       "cleaned_search                                                         \n",
       "accent table                      0          0          0          0   \n",
       "adhesive backsplash               0          0          0          0   \n",
       "adhesive tile backsplash          0          0          0          0   \n",
       "arm chair                         0          0          0          0   \n",
       "armchair                          0          0          0          0   \n",
       "\n",
       "                          100036137  100044505  100048075  100051570  \\\n",
       "cleaned_search                                                         \n",
       "accent table                      0          0          0          0   \n",
       "adhesive backsplash               0          0          0          0   \n",
       "adhesive tile backsplash          0          0          0          0   \n",
       "arm chair                         0          0          0          0   \n",
       "armchair                          0          0          0          0   \n",
       "\n",
       "                          100061089  100074869  ...  307717049  307717219  \\\n",
       "cleaned_search                                  ...                         \n",
       "accent table                      0          0  ...          0          0   \n",
       "adhesive backsplash               0          0  ...          0          0   \n",
       "adhesive tile backsplash          0          0  ...          0          0   \n",
       "arm chair                         0          0  ...          0          0   \n",
       "armchair                          0          0  ...          0          0   \n",
       "\n",
       "                          307717221  307727052  307833295  307920434  \\\n",
       "cleaned_search                                                         \n",
       "accent table                      0          0          0          0   \n",
       "adhesive backsplash               0          0          0          0   \n",
       "adhesive tile backsplash          0          0          0          0   \n",
       "arm chair                         0          0          0          0   \n",
       "armchair                          0          0          0          0   \n",
       "\n",
       "                          307939445  307939707  307940057  307940314  \n",
       "cleaned_search                                                        \n",
       "accent table                      0          0          0          0  \n",
       "adhesive backsplash               0          0          0          0  \n",
       "adhesive tile backsplash          0          0          0          0  \n",
       "arm chair                         0          0          0          0  \n",
       "armchair                          0          0          0          0  \n",
       "\n",
       "[5 rows x 4820 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(imp_matrix.toarray(), index=search_imp_clean.index, columns=vec_imp.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:04:35.540267Z",
     "start_time": "2019-07-21T22:04:35.497494Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100012014</th>\n",
       "      <th>100022800</th>\n",
       "      <th>100023109</th>\n",
       "      <th>100023973</th>\n",
       "      <th>100036137</th>\n",
       "      <th>100044505</th>\n",
       "      <th>100048075</th>\n",
       "      <th>100051570</th>\n",
       "      <th>100061089</th>\n",
       "      <th>100074869</th>\n",
       "      <th>...</th>\n",
       "      <th>307717049</th>\n",
       "      <th>307717219</th>\n",
       "      <th>307717221</th>\n",
       "      <th>307727052</th>\n",
       "      <th>307833295</th>\n",
       "      <th>307920434</th>\n",
       "      <th>307939445</th>\n",
       "      <th>307939707</th>\n",
       "      <th>307940057</th>\n",
       "      <th>307940314</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_search</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accent table</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive backsplash</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhesive tile backsplash</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arm chair</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armchair</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          100012014  100022800  100023109  100023973  \\\n",
       "cleaned_search                                                         \n",
       "accent table                    0.0        0.0        0.0        0.0   \n",
       "adhesive backsplash             0.0        0.0        0.0        0.0   \n",
       "adhesive tile backsplash        0.0        0.0        0.0        0.0   \n",
       "arm chair                       0.0        0.0        0.0        0.0   \n",
       "armchair                        0.0        0.0        0.0        0.0   \n",
       "\n",
       "                          100036137  100044505  100048075  100051570  \\\n",
       "cleaned_search                                                         \n",
       "accent table                    0.0        0.0        0.0        0.0   \n",
       "adhesive backsplash             0.0        0.0        0.0        0.0   \n",
       "adhesive tile backsplash        0.0        0.0        0.0        0.0   \n",
       "arm chair                       0.0        0.0        0.0        0.0   \n",
       "armchair                        0.0        0.0        0.0        0.0   \n",
       "\n",
       "                          100061089  100074869  ...  307717049  307717219  \\\n",
       "cleaned_search                                  ...                         \n",
       "accent table                    0.0        0.0  ...        0.0        0.0   \n",
       "adhesive backsplash             0.0        0.0  ...        0.0        0.0   \n",
       "adhesive tile backsplash        0.0        0.0  ...        0.0        0.0   \n",
       "arm chair                       0.0        0.0  ...        0.0        0.0   \n",
       "armchair                        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "                          307717221  307727052  307833295  307920434  \\\n",
       "cleaned_search                                                         \n",
       "accent table                    0.0        0.0        0.0        0.0   \n",
       "adhesive backsplash             0.0        0.0        0.0        0.0   \n",
       "adhesive tile backsplash        0.0        0.0        0.0        0.0   \n",
       "arm chair                       0.0        0.0        0.0        0.0   \n",
       "armchair                        0.0        0.0        0.0        0.0   \n",
       "\n",
       "                          307939445  307939707  307940057  307940314  \n",
       "cleaned_search                                                        \n",
       "accent table                    0.0        0.0        0.0        0.0  \n",
       "adhesive backsplash             0.0        0.0        0.0        0.0  \n",
       "adhesive tile backsplash        0.0        0.0        0.0        0.0  \n",
       "arm chair                       0.0        0.0        0.0        0.0  \n",
       "armchair                        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 4820 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfimp_matrix.toarray(), index=search_imp_clean.index, columns=tfvec_imp.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:05:00.483061Z",
     "start_time": "2019-07-21T22:05:00.475522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "term_sim = cosine_similarity(imp_matrix,imp_matrix)\n",
    "term_sim_tf = cosine_similarity(tfimp_matrix,tfimp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:13:40.115002Z",
     "start_time": "2019-07-21T22:13:40.110990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Make term - ID dictionary\n",
    "\n",
    "term2id = {}\n",
    "for i,t in enumerate(search_imp_clean.index):\n",
    "    term2id[t] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T02:50:04.659827Z",
     "start_time": "2019-07-22T02:50:04.652909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def topn_terms(term, n=1):\n",
    "    index_num = term2id[term]\n",
    "    \n",
    "    term = search_imp_clean.index[index_num]\n",
    "    \n",
    "    print(f\"Search Term: {term}\")\n",
    "    \n",
    "    cv_result = pd.DataFrame(term_sim, index=search_imp_clean.index, columns=search_imp_clean.index).iloc[:,index_num].sort_values(ascending=False).head(n)\n",
    "    tfidf_result = pd.DataFrame(term_sim_tf, index=search_imp_clean.index, columns=search_imp_clean.index).iloc[:,index_num].sort_values(ascending=False).head(n)\n",
    "    return cv_result, tfidf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T02:13:23.792986Z",
     "start_time": "2019-07-22T02:12:31.975009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../model/embedding/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:37:19.690923Z",
     "start_time": "2019-07-21T20:37:19.683359Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:46:53.007566Z",
     "start_time": "2019-07-19T06:46:52.992194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_vec, search_matrix = vectorize(clean_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:47:22.808035Z",
     "start_time": "2019-07-19T06:47:22.799545Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<354x199 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 698 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:06:18.438716Z",
     "start_time": "2019-07-19T07:06:15.739883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prod_vec, prod_matrix = vectorize(names_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:03.754967Z",
     "start_time": "2019-07-19T07:51:03.742996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "        \n",
    "    def preprocess(self, corpus):\n",
    "        '''\n",
    "        Process documents\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * corpus - list of documents\n",
    "        * stop_words = set of stopwords\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Each document as a list of tokens (iterator)\n",
    "\n",
    "        e.g., clean_corpus = list(preprocess(corpus))\n",
    "        '''\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        for doc in corpus:\n",
    "            tokens = gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "            yield([lemmatizer.lemmatize(token) for token in tokens if not token in stop_words])\n",
    "    \n",
    "    \n",
    "    def trigram_model(self, corpus_tokens, threshholds=(25,15), verbose=False):\n",
    "        '''\n",
    "        Build trigram model\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * corpus - list of documents (as tokens)\n",
    "        * stop_words = set of stopwords\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        '''\n",
    "\n",
    "        bigram = gensim.models.Phrases(corpus_tokens,\n",
    "                                        min_count=threshholds[0])\n",
    "\n",
    "        trigram = gensim.models.Phrases(bigram[corpus_tokens],\n",
    "                                        min_count=threshholds[1])\n",
    "\n",
    "        # trigram/bigram model\n",
    "        bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        corpus_new = [trigram_model[bigram_model[doc]] for doc in corpus_tokens]\n",
    "\n",
    "        if verbose:\n",
    "            for doc in corpus_new[0:5]:\n",
    "                print(f'{\" \".join(trigram_model[bigram_model[doc]]) } \\n')\n",
    "\n",
    "        return corpus_new\n",
    "                      \n",
    "    def vectorize(self, corpus, format='tokens'):\n",
    "                      \n",
    "        if format == 'tokens':    \n",
    "            corpus = self.tokens2corpus(corpus)\n",
    "    \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        return vectorizer, matrix\n",
    "\n",
    "\n",
    "    def tokens2corpus(self, tokenized_docs):\n",
    "        '''\n",
    "        Helper fuction to convert from tokens to text\n",
    "        '''\n",
    "        for doc in tokenized_docs:\n",
    "            yield ' '.join([token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Testing Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:04.706363Z",
     "start_time": "2019-07-19T07:51:04.701928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tp = TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:05.201313Z",
     "start_time": "2019-07-19T07:51:05.198153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search = tp(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:06.610371Z",
     "start_time": "2019-07-19T07:51:06.597143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_searches = list(search.preprocess(searches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:08.187428Z",
     "start_time": "2019-07-19T07:51:08.164037Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee \n",
      "\n",
      "bag chair \n",
      "\n",
      "kitchen wall tile \n",
      "\n",
      "mirror tile \n",
      "\n",
      "entryway \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jaime/miniconda3/envs/thddecor/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "searches_trigram = search.trigram_model(clean_searches, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T07:51:13.114489Z",
     "start_time": "2019-07-19T07:51:13.106608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "searches_vec, searches_matrix = search.vectorize(searches_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**To Explore**\n",
    "- Language detection \n",
    "- Snowball stemmer vs Lemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Build the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trigram_model(corpus, threshholds=(25,15), verbose=False):\n",
    "    '''\n",
    "    Build trigram model\n",
    "    '''\n",
    "\n",
    "    bigram = gensim.models.Phrases(clean_names,\n",
    "                                    min_count=threshholds[0])\n",
    "\n",
    "    trigram = gensim.models.Phrases(bigram[clean_names],\n",
    "                                    min_count=threshholds[1])\n",
    "\n",
    "    # trigram/bigram model\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    corpus_new = [trigram_model[bigram_model[doc]] for doc in corpus]\n",
    "\n",
    "    if verbose:\n",
    "        for doc in corpus[0:5]:\n",
    "            print(f'{\" \".join(trigram_model[bigram_model[doc]]) } \\n')\n",
    "                  \n",
    "    return corpus_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(clean_names, min_count=25)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[clean_names], min_count=15) \n",
    "\n",
    "# Names as a trigram/bigram\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Preprocess Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add Stopword List\n",
    "# Define List of Stop Words\n",
    "new_stop_words = ['in', 'sq','ft', 'yd', 'cm', 'mm','gal','lb' ,'lbs','qt','oz', 'h', 'w', 'ii', 'x']\n",
    "stop_words = set(stopwords.words('english') + new_stop_words)\n",
    "\n",
    "# Convert Product Name to Array\n",
    "names = prod_desc['Product_name'].values\n",
    "\n",
    "clean_names = list(preprocess_text(names))\n",
    "\n",
    "names_corpus = list(join_tokens(clean_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Text Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T07:48:32.468576Z",
     "start_time": "2019-07-22T07:48:32.457714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, search_imp=None, prod_desc=None, catalog=None):\n",
    "        self.search_imp = search_imp\n",
    "        self.prod_desc = prod_desc\n",
    "        self.catalog = catalog\n",
    "        \n",
    "        self.stop_words_lst = ['in', 'sq','ft', 'yd', 'cm', 'mm','gal','lb' ,'lbs','qt','oz', 'h', 'w', 'ii', 'x']\n",
    "        self.stop_words = list(set(stopwords.words('english') + new_stop_words))\n",
    "        \n",
    "    def read_in_dataset(self, data, data_folder='raw', verbose=False):\n",
    "        '''\n",
    "        Read in dataset (csv format) to pandas dataframe\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * dataset - string with dataset filename\n",
    "        * data_folder - string with either raw or processed\n",
    "        * verbose - True will print intormation about the dataset\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        a pandas dataframe\n",
    "        '''\n",
    "        df = pd.read_csv('../data/{}/{}.csv'.format(data_folder, data))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def clean_text(self, data, corpus_col):\n",
    "        '''Call preprocessor generator object'''\n",
    "        \n",
    "#         if data == 'prod_desc':\n",
    "#             corpus = self.prod_desc[corpus_col].tolist()\n",
    "        \n",
    "#         elif data == 'search_imp':\n",
    "#             corpus = self.search_imp[corpus_col].tolist()\n",
    "        \n",
    "#         elif data == 'catalog':\n",
    "#             corpus = self.catalog[corpus_col].tolist()\n",
    "        corpus = data.loc[:, corpus_col].tolist()\n",
    "        clean_corpus = list(self.preprocess_text(corpus))\n",
    "        \n",
    "        return clean_corpus\n",
    "        \n",
    "\n",
    "    def preprocess_text(docs, method='lemmatizer'):\n",
    "        '''\n",
    "        Process docs\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tokenized list of docs\n",
    "        '''\n",
    "        #docs = docs.values\n",
    "        \n",
    "        if method == 'lemmatizer':\n",
    "            lemma = nltk.stem.WordNetLemmatizer()\n",
    "            root = lemma.lemmatize\n",
    "        elif method == 'stemmer':\n",
    "            stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "            root = stemmer.stem\n",
    "\n",
    "        for doc in docs:\n",
    "            tokens = gensim.utils.simple_preprocess(doc)\n",
    "            yield(' '.join([root(token) for token in tokens if not token in self.stop_words]))\n",
    "        \n",
    "    def doc2tokens(corpus):\n",
    "        return corpus.str.split()\n",
    "\n",
    "    def add_stopword(self, new_stopword):\n",
    "        self.stop_words.append(new_stopword)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Preprocess Product Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:00:15.585780Z",
     "start_time": "2019-07-16T21:00:12.331261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Make names df\n",
    "names_df= prod_desc_cat.loc[:, ['SKU_ID','Product_name']]\n",
    "names_df.set_index('Product_name', inplace=True)\n",
    "names_df['clean_names'] = clean_names\n",
    "names_df['clean_names'] = names_df['clean_names'].str.join(' ')\n",
    "\n",
    "\n",
    "#Make queries df\n",
    "queries_df = search_prod_levels.loc[:, ['Search_term']]\n",
    "queries_df['clean_queries'] = clean_queries\n",
    "queries_df = queries_df.drop_duplicates('Search_term')\n",
    "queries_df.set_index('Search_term', inplace=True)\n",
    "queries_df['clean_queries'] = queries_df['clean_queries'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:00:31.022546Z",
     "start_time": "2019-07-16T21:00:21.417756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words=stop_words)\n",
    "tfidf_matrix_names = tf.fit_transform(names_df['clean_names'])\n",
    "tfidf_matrix_queries = tf.fit_transform(queries_df['clean_queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-16T21:07:55.425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Cosine similarity\n",
    "cosine_similarities = linear_kernel(tfidf_matrix_names, tfidf_matrix_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(names_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "recommendations('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create Preprocessor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "    def __init__(self, cols_to_filter=None):\n",
    "        \n",
    "        self.cols_to_filter = cols_to_filter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"learn any information from the training data we may need to transform the test data\"\"\"\n",
    "        \n",
    "        # learn from the training data and return the class itself. \n",
    "        # allows you to chain fit and predict methods like \n",
    "        \n",
    "        # > p = preprocessor()\n",
    "        # > p.fit(X).transform(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"transform the training or test data\"\"\"\n",
    "        # transform the training or test data based on class attributes learned in the `fit` step\n",
    "        return X_new"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:thddecor]",
   "language": "python",
   "name": "conda-env-thddecor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
