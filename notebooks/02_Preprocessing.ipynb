{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Home Depot Decor - Visual Navigation\n",
    "\n",
    "Getting Started | Data Exploration  |  **Preprocessing**  | Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T06:51:01.940296Z",
     "start_time": "2019-07-25T06:51:01.934552Z"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = 'Jaime Garvey'\n",
    "__email__ = 'jaimemgarvey@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T07:26:02.392724Z",
     "start_time": "2019-07-22T07:26:02.385916Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T20:51:58.046165Z",
     "start_time": "2019-07-21T20:51:58.042097Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "sys.path.insert(0, '../modules')\n",
    "\n",
    "\n",
    "# now read in new functions\n",
    "from helpers import read_in_dataset, get_num_of_levels, flatten_categories, search_cons_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T06:20:40.956399Z",
     "start_time": "2019-07-22T06:20:38.679225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in product related data\n",
    "verbose_opt = False\n",
    "catalog = read_in_dataset('Decor_catalog.csv', verbose=verbose_opt)\n",
    "prod_desc = read_in_dataset('Product_name_description.csv', verbose=verbose_opt)\n",
    "prod_engagement = read_in_dataset('Product_engagement.csv', verbose=verbose_opt)\n",
    "\n",
    "# Read in search related data\n",
    "navigations = read_in_dataset('Visual_navigations.csv', verbose=verbose_opt)\n",
    "search_imp = read_in_dataset('Search_impression.csv', verbose=verbose_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T06:20:57.526305Z",
     "start_time": "2019-07-22T06:20:57.517895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search_term</th>\n",
       "      <th>Impression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>203054703;207061099;305561354;305561469;301692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bag chair</td>\n",
       "      <td>305573411;305608772;301092388;301092383;301092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen wall tile</td>\n",
       "      <td>205140711;302603437;205762409;204923728;204337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mirror tile</td>\n",
       "      <td>305696621;304142073;304142126;304142039;303058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entryway</td>\n",
       "      <td>203532713;203532652;300750153;302042988;302042...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Search_term                                         Impression\n",
       "0             coffee  203054703;207061099;305561354;305561469;301692...\n",
       "1          bag chair  305573411;305608772;301092388;301092383;301092...\n",
       "2  kitchen wall tile  205140711;302603437;205762409;204923728;204337...\n",
       "3        mirror tile  305696621;304142073;304142126;304142039;303058...\n",
       "4           entryway  203532713;203532652;300750153;302042988;302042..."
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_imp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:13:23.437017Z",
     "start_time": "2019-07-22T08:13:23.434042Z"
    }
   },
   "source": [
    "# Build Text Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:26:29.441401Z",
     "start_time": "2019-07-22T08:26:29.425633Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, search_imp=None, prod_desc=None, catalog=None):\n",
    "        self.search_imp = search_imp\n",
    "        self.prod_desc = prod_desc\n",
    "        self.catalog = catalog\n",
    "        \n",
    "        self.stop_words_lst = ['in', 'sq','ft', 'yd', 'cm', 'mm','gal','lb' ,'lbs','qt','oz', 'h', 'w', 'ii', 'x']\n",
    "        self.stop_words = list(set(stopwords.words('english') + new_stop_words))\n",
    "        \n",
    "    def read_in_dataset(self, data, data_folder='raw', verbose=False):\n",
    "        '''\n",
    "        Read in dataset (csv format) to pandas dataframe\n",
    "\n",
    "        Keyword Arguments:\n",
    "        ------------------\n",
    "        * dataset - string with dataset filename\n",
    "        * data_folder - string with either raw or processed\n",
    "        * verbose - True will print intormation about the dataset\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        a pandas dataframe\n",
    "        '''\n",
    "        df = pd.read_csv('../data/{}/{}.csv'.format(data_folder, data))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def clean_text(self, df, corpus_col):\n",
    "        '''Call preprocessor generator object'''\n",
    "\n",
    "        corpus = df.loc[:, corpus_col].tolist()\n",
    "        \n",
    "        return list(self.preprocess_text(corpus))\n",
    "        \n",
    "\n",
    "    def preprocess_text(self, docs):\n",
    "        '''\n",
    "        Process docs\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tokenized list of docs\n",
    "        '''\n",
    "        #docs = docs.values\n",
    "        method='lemmatizer'\n",
    "        if method == 'lemmatizer':\n",
    "            lemma = nltk.stem.WordNetLemmatizer()\n",
    "            root = lemma.lemmatize\n",
    "        elif method == 'stemmer':\n",
    "            stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "            root = stemmer.stem\n",
    "\n",
    "        for doc in docs:\n",
    "            tokens = gensim.utils.simple_preprocess(doc)\n",
    "            yield(' '.join([root(token) for token in tokens if not token in self.stop_words]))\n",
    "        \n",
    "    def doc2tokens(corpus):\n",
    "        return corpus.str.split()\n",
    "    \n",
    "    def compare_clean_searches(self, clean_searches, raw_search):\n",
    "        '''Compare cleaned search queries to raw text'''\n",
    "        st_compare = pd.DataFrame({'raw_search': raw_search['Search_term'], 'cleaned_search':clean_searches}).sort_values(by='raw_search')\n",
    "        return st_compare.groupby('cleaned_search')['raw_search'].apply(list)\n",
    "\n",
    "    def add_stopword(self, new_stopword):\n",
    "        self.stop_words.append(new_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Search Term Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Preprocess Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(docs, method='stemmer'):\n",
    "    '''\n",
    "    Process docs\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tokenized list of docs\n",
    "    '''\n",
    "    docs = docs.values\n",
    "    stop_words = ['I']\n",
    "    if method == 'lemmatizer':\n",
    "        lemma = nltk.stem.WordNetLemmatizer()\n",
    "        root = lemma.lemmatize\n",
    "        \n",
    "    elif method == 'stemmer':\n",
    "        stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "        root = stemmer.stem\n",
    "\n",
    "    for doc in docs:\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        yield(' '.join([root(token) for token in tokens if not token in stop_words]))\n",
    "        \n",
    "        \n",
    "def doc2tokens(corpus):\n",
    "    '''pandas series (docs) to tokens'''\n",
    "        return corpus.str.split()\n",
    "    \n",
    "    \n",
    "def cons_search_terms(clean_searches=clean_searches, raw_search=search_imp):\n",
    "    '''Compare cleaned search queries to raw text'''\n",
    "    st_compare = pd.DataFrame({'raw_search': raw_search['Search_term'], 'cleaned_search':clean_searches}).sort_values(by='raw_search')\n",
    "    return st_compare.groupby('cleaned_search')['raw_search'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T07:56:06.014424Z",
     "start_time": "2019-07-22T07:56:05.937261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean Search Terms\n",
    "clean_searches = pd.Series(list(preprocess_text(search_imp['Search_term'])))\n",
    "\n",
    "# tokens\n",
    "search_tokens = doc2tokens(clean_searches)\n",
    "\n",
    "#ngram model\n",
    "search_trigrams = trigram_model(search_tokens, threshholds=(25,15), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:02:51.724074Z",
     "start_time": "2019-07-22T08:02:51.682070Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_search\n",
       "accent table                [accent table, accent tables]\n",
       "adhesive backsplash                 [adhesive backsplash]\n",
       "adhesive tile backsplash       [adhesive tile backsplash]\n",
       "arm chair                                     [arm chair]\n",
       "armchair                                       [armchair]\n",
       "Name: raw_search, dtype: object"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare clean searches to raw searches\n",
    "\n",
    "cons_search_terms().head()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:thddecor]",
   "language": "python",
   "name": "conda-env-thddecor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
